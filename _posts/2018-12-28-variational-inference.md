---
layout: post
comments: true
title:  "Variational Inference"
date:   2018-12-28
permalink: /posts/2013/08/variational-inference/
tags:
  - cool posts
  - category1
  - category2
---
In probabilistic machine learning we are always assuming that our observations $$ \{x_1,x_2,\ldots,x_N\} $$ are samples generated from probability distributions and we are dealing with their joint probability:

$$
\begin{equation}
p(x[0],x[1],\ldots,x[N-1];\theta)
\end{equation}
$$

For each particular problem 
(or dataset$$$$)
 we can define our model for this joint probability. 
when the notation used for the probability distribution has semicolon (i.e. $$p(x;\theta)$$), it means that in our predefined model we have a deterministic but unknwon parameter $$\theta$$ 
<!-- that by using an approach such as maximum likelihood this parameter (or a set of parameters$$$$) can be found. -->
and we are looking for an estimator (a function of samples$$$$)
to
 approximate this parameter
:


$$
\hat{\theta}=g(x[0],x[1],\ldots,x[N-1])
$$

For example in 
language modeling we can assume words in a sentence are i.i.d data samples  (the sentence is an array of these words $$s=\{w[0],w[1],\ldots,w[N-1] \}$$) and our aim is to find the parameters of their distribution:

$$
w[i]\sim p(w;\theta)
$$

We can improve this model by adding latent variables.  In language modeling we know that sentences are generated by grammar rules, so this prior information can help us to make more accurate models. In other words, in this new model first there are samples generated from hidden variables ($$\mathbf{z}$$) and then our observations ($$\mathbf{x}$$) are generated by them.
